== Intent ==

parallel processing with [[Technology:Hadoop]]

== Languages ==

* [[Language:Java]]

== Technologies ==

* [[Technology:Hadoop]]
* [[101profile:Simple Java]]

== Features ==

* [[101feature:Company]]
* [[101feature:Total]]
* [[101feature:Cut]]
* [[101feature:Serialization]]
* [[101feature:Parallelism]]

== Motivation ==

Companies are processed in parallel manner according to the [[MapReduce]] programming 
model. To this end, primitive data of companies, departments, and employees can be stored
in [[file]]s of fixed-size [[record]]s in a [[:Category:distributed file system]].

== Illustration ==

While data is stored in files, the records can be de-serialized into objects
to be presented to MapReduce functionality. For instance, an object type for
employees is designed as follows:

<syntaxhighlight lang="java">
class Employee
   implements WritableComparable<Employee> {

	private Text name;
	private Text address;
	private DoubleWritable salary;
	private Text company;

   // getters, setters and omitted
    
   public void readFields(DataInput in)
         throws IOException {
      name = new Text();
      name.readFields(in);
      address = new Text();
      address.readFields(in);
      ...
   }
}
</syntaxhighlight>

That is, there are properties for name, address, and salary---as usual. In
addition, there is a property for the the company so that the company of each
employee is immediately known without any traversal or state-based effort. 
Objects are populated from records on file through a ''readFields'' method
that is required for any deserializable type.

A MapReduce computation consists of a mapper and a reducer. The 
essential methods of these components, i.e., methods ''map'' (extraction)
and ''reduce'' (aggregation) are shown below:

<syntaxhighlight lang="java">
protected void map(
   Text key, Employee value, Context context)
   throws ... {
      context.write(
         value.getCompany(),
         value.getSalary());
}
</syntaxhighlight>

<syntaxhighlight lang="java">
protected void reduce(
   Text key, Iterable<...> values, Context context)
   throws ... {
      double total = 0;
      for(DoubleWritable value: values)
         total += value.get();
      context.write(key, new DoubleWritable(total));
}       
</syntaxhighlight>

That is, the ''map'' method constructs an intermediate key-value
pair from each employee such that it pairs up the salary of the
employee with its company (say, its name) as the key. This choice
of key implies that the MapReduce framework will correctly group
together all salaries per company. Hence, the ''reduce'' method
simply iterates over all salaries, grouped by key, and sums them 
up by a trivial aggregation loop; the company key together with 
the total of salaries is written to the output file.

== Architecture ==

Package ''org.softlang.company'' hosts the object model for [[101feature:Company]].
Package ''org.softlang.operations'' hosts designated classes with static methods for
the MapReduce jobs [[101feature:Total]] and [[101feature:Cut]]. Some boilerplate code for 
[[101feature:Serialization]] is implemented in the class ''org.softlang.company.Company'' 
(see methods ''readObject'' and ''writeObject''). Package ''org.softlang.tests'' 
hosts JUnit tests; see below.

== Usage ==

* The implementation is provided as an Eclipse project.
* Hence, open the project with Eclipse; this will also build the project.
* The default settings runs Hadoop on your local machine. For distributed setup see below.
* There are JUnit tests available as the package ''org.softlang.tests''.
** Run class ''Serialization'' with JUnit to create and serialize an example Company.
** Run class ''Basics'' with JUnit to exercise basic features.

== Distributed Setup ==

An official release for Hadoop can be downloaded here: http://hadoop.apache.org/common/releases.html
The Hadoop Wiki describes how to set up Hadoop in local (http://hadoop.apache.org/common/docs/current/single_node_setup.html)
and distributed mode (http://hadoop.apache.org/common/docs/current/cluster_setup.html).

These configurations require jobs being run via the command line using a jar file. The jar can be build
from our Eclipse project by simply specifying the class ''OperationRunner'' as main class. 

Using Hadoop in distributed mode under Eclipse is not that trivial. Instructions can be found here:
http://wiki.apache.org/hadoop/EclipseEnvironment

== Possible issues ==

Running Hadoop under Windows requires ''Cygwin'' being installed. Instructions can be found here:
http://wiki.apache.org/hadoop/GettingStartedWithHadoop

Hadoop also requires ''ssh'' to access all machines (including ''localhost'') without password prompt.
Instructions how to do this can be found for example here: http://www.maths.qmul.ac.uk/~dhruba/tips_and_tricks/node2.html

== Contributors ==

* {{101contributor|David Saile|developer}}
* {{101contributor|Ralf LÃ¤mmel|reviewer}}