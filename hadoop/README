== Intent ==

parallel processing with [[Technology:Hadoop]]

== Languages ==

* [[Language:Java]]

== Technologies ==

* [[Technology:Hadoop]]
* [[Javaclipse]]

== Features ==

* [[101feature:Company]]
* [[101feature:Total]]
* [[101feature:Cut]]
* [[101feature:Serialization]]
* [[101feature:Parallelism]]

== Motivation ==

Companies are processed in parallel manner with [[Technology:Hadoop]]. To this end, 
companies, departments, and  employees are represented as [[record]]s of [[file]]s 
that are stored in a [[:Category:distributed file system]]. Computations on these 
files are then to be described according to the [[MapReduce]] programming model.

== Illustration ==

Data is primarily stored in files, but records from such files can be presented
to the MapReduce functionality as objects. For instance, an object type for
employees is designed as follows:

<syntaxhighlight lang="java">
class Employee
   implements WritableComparable<Employee> {

	private Text name;
	private Text address;
	private DoubleWritable salary;
	private Text company;

   // getters, setters and omitted
    
   public void readFields(DataInput in)
         throws IOException {
      name = new Text();
      name.readFields(in);
      address = new Text();
      address.readFields(in);
      ...
   }
}
</syntaxhighlight>

That is, there are properties for name, address, and salary---as usual. In
addition, there is a property for the the company so that the company of each
employee is immediately known without any traversal or state-based effort. 
Objects are populated from records on file through a ''readFields'' method
that is required for any type used on file.

A MapReduce computation consists of a mapper and a reducer. The essence
of these ingredients are shown below, i.e., the corresponding methods 
''map'' (extraction) and ''reduce'' (aggregation):

<syntaxhighlight lang="java">
protected void map(
   Text key, Employee value, Context context)
   throws ... {
      context.write(
         value.getCompany(),
         value.getSalary());
}
</syntaxhighlight>

<syntaxhighlight lang="java">
protected void reduce(
   Text key, Iterable<...> values, Context context)
   throws ... {
      double total = 0;
      for(DoubleWritable value: values)
         total += value.get();
      context.write(key, new DoubleWritable(total));
}       
</syntaxhighlight>

That is, the ''map'' method constructs an intermediate key-value
pair from each employee such that it pairs up the salary of the
employee with its company (say, its name) as the key. This choice
of key implies that the MapReduce framework will correctly group
together all salaries per company. Hence, the ''reduce'' method
simply iterates over all salaries and sums the up by a trivial 
aggregation loop; the company key together with the total of 
salaries is written to the output file.

== Architecture ==

Package ''org.softlang.company'' hosts the object model for [[101feature:Company]].
Package ''org.softlang.operations'' hosts designated classes with static methods for
the MapReduce jobs [[101feature:Total]] and [[101feature:Cut]]. Some boilerplate code for 
[[101feature:Serialization]] is implemented in the class ''org.softlang.company.Company'' 
(see methods ''readObject'' and ''writeObject''). Package ''org.softlang.tests'' 
hosts JUnit tests; see below.

== Usage ==

* The implementation is provided as an Eclipse project.
* Hence, open the project with Eclipse; this will also build the project.
* The default settings runs Hadoop on your local machine. For distributed setup see below.
* There are JUnit tests available as the package ''org.softlang.tests''.
** Run class ''Serialization'' with JUnit to create and serialize an example Company.
** Run class ''Basics'' with JUnit to exercise basic features.

== Distributed Setup ==

An official release for Hadoop can be downloaded here: http://hadoop.apache.org/common/releases.html
The Hadoop Wiki describes how to set up Hadoop in local (http://hadoop.apache.org/common/docs/current/single_node_setup.html)
and distributed mode (http://hadoop.apache.org/common/docs/current/cluster_setup.html).

These configurations require jobs being run via the command line using a jar file. The jar can be build
from our Eclipse project by simply specifying the class ''OperationRunner'' as main class. 

Using Hadoop in distributed mode under Eclipse is not that trivial. Instructions can be found here:
http://wiki.apache.org/hadoop/EclipseEnvironment

== Possible issues ==

Running Hadoop under Windows requires ''Cygwin'' being installed. Instructions can be found here:
http://wiki.apache.org/hadoop/GettingStartedWithHadoop

Hadoop also requires ''ssh'' to access all machines (including ''localhost'') without password prompt.
Instructions how to do this can be found for example here: http://www.maths.qmul.ac.uk/~dhruba/tips_and_tricks/node2.html

== Contributors ==

* {{101contributor|David Saile|developer}}
* {{101contributor|Ralf LÃ¤mmel|reviewer}}